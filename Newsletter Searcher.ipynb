{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser, FuzzyTermPlugin\n",
    "from whoosh import highlight\n",
    "\n",
    "import calendar\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "NewsFeed = feedparser.parse(\"https://thegeneralist.substack.com/feed\")\n",
    "\n",
    "entry = NewsFeed.entries[0]\n",
    "print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entry['title'])\n",
    "print(entry['author'])\n",
    "print(entry['summary'])\n",
    "print(entry['link'])\n",
    "print(entry['published_parsed'])\n",
    "content = entry['content'][0]['value']\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn HTML into clean text for nice searching\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "tree = BeautifulSoup(content)\n",
    "\n",
    "# This validates and cleans HTML but Substack HTML is already fine\n",
    "# pretty = tree.prettify()\n",
    "# print(pretty)\n",
    "\n",
    "pure_text = tree.get_text(\"\\n\")\n",
    "print(pure_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, build a Whoosh database and search engine\n",
    "# https://whoosh.readthedocs.io/en/latest/quickstart.html#a-quick-introduction\n",
    "\n",
    "# Define schema (pretty simple)\n",
    "# Add documents from all top Substacks\n",
    "# Build a searcher\n",
    "# Show results\n",
    "# Highlight bits from the thinkpieces that match\n",
    "# Optional: add stemming for better searching\n",
    "# Later, add support for other newsletters with an RSS feed\n",
    "# Maybe, later, offer `more_like_this` so people can rabbit-hole in\n",
    "\n",
    "# Use case: I want to find all articles from my favorite writers that include the term\n",
    "# \"BNPL\" since I want to research \"Buy Now, Pay Later\" in fintech.\n",
    "# Or I want to find all thinkpieces that mentioned Google in the last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all RSSes to search\n",
    "\n",
    "# First, Substacks. Getting their RSS feeds is pretty simple/systematic.\n",
    "substack_domains = [\n",
    "    \"thegeneralist\",\n",
    "    \"danco\",\n",
    "    \"diff\",\n",
    "    \"nbt\",\n",
    "    \"platformer\",\n",
    "    \"notboring\",\n",
    "    \"sariazout\",\n",
    "    \"digitalnative\",\n",
    "    \"jamesonstartups\",\n",
    "    \"breakingsmart\",\n",
    "    \"artofgig\",\n",
    "    \"theskip\",\n",
    "    \"gwern\"\n",
    "]\n",
    "\n",
    "# Feeds are, e.g., https://thegeneralist.substack.com/feed\n",
    "substack_feeds = [\"https://{0}.substack.com/feed\".format(domain) for domain in substack_domains]\n",
    "\n",
    "# Now add some custom RSS feeds\n",
    "# Medium feeds are medium.com/feed/@user or medium.com/feed/publication\n",
    "custom_feeds = [\n",
    "    \"https://stratechery.com/feed/\",\n",
    "    \"https://www.profgalloway.com/feed\",\n",
    "    \"https://eugene-wei.squarespace.com/blog?format=rss\",\n",
    "    \"https://medium.com/feed/@superwuster\",\n",
    "    \"https://commoncog.com/blog/rss\",\n",
    "    \"https://www.lennyrachitsky.com/feed\",\n",
    "    \"https://medium.com/feed/bloated-mvp\",\n",
    "    \"https://daringfireball.net/feeds/main\",\n",
    "    \"https://wongmjane.com/api/feed/rss\",\n",
    "    \"https://fourweekmba.com/feed\",\n",
    "]\n",
    "\n",
    "# Unite all feeds into one\n",
    "all_feeds = substack_feeds + custom_feeds\n",
    "all_feeds\n",
    "\n",
    "# We'll read in the feeds from each of these. Get RSS feed at \n",
    "# https://thegeneralist.substack.com/feed\n",
    "\n",
    "# NewsFeed = feedparser.parse(\"https://thegeneralist.substack.com/feed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function to safely get an item from a dict.\n",
    "# If the key doesn't exist, just returns none\n",
    "def safe_get(obj, key):\n",
    "    if obj.has_key(key):\n",
    "        return obj[key]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows optional case-sensitive searches. all-lowercase is case insensitive, \n",
    "# any capital letters makes it case sensitive \n",
    "\n",
    "class CaseSensitivizer(analysis.Filter):\n",
    "    def __call__(self, tokens):\n",
    "        for t in tokens:\n",
    "            yield t\n",
    "            if t.mode == \"index\":\n",
    "               low = t.text.lower()\n",
    "               if low != t.text:\n",
    "                   t.text = low\n",
    "                   yield t\n",
    "\n",
    "ana = analysis.RegexTokenizer() | CaseSensitivizer()\n",
    "# [t.text for t in ana(\"The new SuperTurbo 5000\", mode=\"index\")]\n",
    "# [\"The\", \"the\", \"new\", \"SuperTurbo\", \"superturbo\", \"5000\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the database\n",
    "schema = Schema(\n",
    "    title=TEXT(stored=True),\n",
    "    author=TEXT(stored=True),\n",
    "    publication=TEXT(stored=True),\n",
    "    summary=TEXT(stored=True),\n",
    "    url=TEXT(stored=True),\n",
    "    published=DATETIME(stored=True, sortable=True),\n",
    "    content=TEXT(stored=True, analyzer=ana))\n",
    "index = create_in(\"whoosh_index2\", schema)\n",
    "writer = index.writer()\n",
    "\n",
    "# Read every item from our RSS feeds into there\n",
    "# Call writer.add_document() repeatedly for each item \n",
    "\n",
    "for feed_url in all_feeds:\n",
    "    print(feed_url)\n",
    "    news_feed = feedparser.parse(feed_url)\n",
    "    \n",
    "    # NOTE: we can only get the last few entries from this RSS feed.\n",
    "    # Substack doesn't seem to show anything older than the last 20.\n",
    "    # So we should build in a system to start caching these.\n",
    "    \n",
    "#     print(len(news_feed.entries))\n",
    "\n",
    "    for entry in news_feed.entries:\n",
    "        \n",
    "        # Get publication name. This is in the feed's `feed` field, along with other metadata\n",
    "        publication = None\n",
    "        metadata = safe_get(news_feed, 'feed')\n",
    "        if metadata is not None:\n",
    "            publication = safe_get(metadata, 'title')\n",
    "\n",
    "        # Clean up the date into a normal datetime\n",
    "        clean_datetime = datetime.fromtimestamp(calendar.timegm(entry['published_parsed']))\n",
    "        \n",
    "        # Most feeds put the main content in `content`,\n",
    "        # but a rare few like Eugene Wei put it in `summary`\n",
    "        # (in which case `content` is empty). With this logic, let's get a single `content` field.\n",
    "        body_text = None\n",
    "        # See if `content` exists\n",
    "        content_holder = safe_get(entry, 'content')\n",
    "        if content_holder is not None:\n",
    "            # We have content; fill it in\n",
    "            content_tree = BeautifulSoup(content_holder[0]['value'])\n",
    "            body_text = content_tree.get_text(\" \", strip=True)\n",
    "        else:\n",
    "            # No content provided. `summary` must hold all the text.\n",
    "            summary_tree = BeautifulSoup(safe_get(entry, 'summary'))\n",
    "            body_text = summary_tree.get_text(\" \", strip=True)\n",
    "\n",
    "        writer.add_document(\n",
    "            title=safe_get(entry, 'title'),\n",
    "            author=safe_get(entry, 'author'),\n",
    "            publication=publication,\n",
    "            summary=safe_get(entry, 'summary'),\n",
    "            url=safe_get(entry, 'link'),\n",
    "            published=clean_datetime,\n",
    "            content=body_text)\n",
    "\n",
    "print(\"DONE!\")\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, we're overriding the standard fragment formatter\n",
    "class BracketFormatter(highlight.Formatter):\n",
    "    \"\"\"Puts square brackets around the matched terms.\n",
    "    \"\"\"\n",
    "\n",
    "    def format_token(self, text, token, replace=False):\n",
    "        # Use the get_text function to get the text corresponding to the\n",
    "        # token\n",
    "        tokentext = highlight.get_text(text, token, replace)\n",
    "\n",
    "        # Return the text as you want it to appear in the highlighted\n",
    "        # string\n",
    "        return \"[[%s]]\" % tokentext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try searching\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "import whoosh.qparser as qparser\n",
    "\n",
    "search_term = \"Notion\"\n",
    "\n",
    "with index.searcher() as searcher:\n",
    "    parser = QueryParser(\"content\", index.schema)\n",
    "    # Allow fuzzy matching (EDIT: kinda screws things up)\n",
    "    # parser.add_plugin(FuzzyTermPlugin())\n",
    "    # Allow searching for entire phrases w/ single quotes, like 'microsoft teams'\n",
    "    parser.add_plugin(qparser.SingleQuotePlugin())\n",
    "    \n",
    "    query = parser.parse(search_term)\n",
    "    results = searcher.search(query, limit=None)\n",
    "    \n",
    "    # Highlighting settings\n",
    "    # This provides more context characters around the searched-for text\n",
    "    results.fragmenter.surround = 50\n",
    "    results.fragmenter.maxchars = 500\n",
    "    \n",
    "    # Surround matched tags with brackets\n",
    "    results.formatter = BracketFormatter()\n",
    "    \n",
    "    # Convert each Hit into a dict\n",
    "    def extract_hit_info(hit):\n",
    "        return {\n",
    "            'title': hit.get('title'),\n",
    "            'publication': hit.get('publication'),\n",
    "            'author': hit.get('author'),\n",
    "            'url': hit.get('url'),\n",
    "            'highlights': hit.highlights(\"content\", top=3),\n",
    "            'published': hit.get('published'),\n",
    "            'score': hit.score\n",
    "        }\n",
    "    \n",
    "    hit_list = [extract_hit_info(h) for h in results]\n",
    "    \n",
    "    print(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental\n",
    "\n",
    "news_feed = feedparser.parse(\"https://diff.substack.com/feed\")\n",
    "print(news_feed['feed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: read from the thinkpiecer module and use it here, instead of writing custom code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = {'a':5, 'b':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.get('c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the newest items.\n",
    "\n",
    "# Try searching\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "import whoosh.qparser as qparser\n",
    "from whoosh.qparser.dateparse import DateParserPlugin\n",
    "\n",
    "search_term = \"'-26 weeks to now'\"\n",
    "\n",
    "with index.searcher() as searcher:\n",
    "    parser = QueryParser(\"published\", index.schema)\n",
    "    # Allow fuzzy matching (EDIT: kinda screws things up)\n",
    "    # parser.add_plugin(FuzzyTermPlugin())\n",
    "    # Allow searching for entire phrases w/ single quotes, like 'microsoft teams'\n",
    "    parser.add_plugin(qparser.SingleQuotePlugin())\n",
    "    \n",
    "    # Add the DateParserPlugin to the parser\n",
    "    parser.add_plugin(DateParserPlugin())\n",
    "    \n",
    "    query = parser.parse(search_term)\n",
    "    results = searcher.search(query, limit=50, sortedby=\"published\", reverse=True)\n",
    "    \n",
    "    # Highlighting settings\n",
    "    # This provides more context characters around the searched-for text\n",
    "#     results.fragmenter.surround = 50\n",
    "#     results.fragmenter.maxchars = 500\n",
    "    \n",
    "    # Surround matched tags with brackets\n",
    "#     results.formatter = BracketFormatter()\n",
    "    \n",
    "    # Convert each Hit into a dict\n",
    "    def extract_hit_info(hit):\n",
    "        return {\n",
    "            'title': hit.get('title'),\n",
    "            'publication': hit.get('publication'),\n",
    "            'author': hit.get('author'),\n",
    "            'url': hit.get('url'),\n",
    "#             'highlights': hit.highlights(\"content\", top=3),\n",
    "            'published': hit.get('published'),\n",
    "            'score': hit.score\n",
    "        }\n",
    "    \n",
    "    hit_list = [extract_hit_info(h) for h in results]\n",
    "    \n",
    "    print(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': \"Why labels on misinformation aren't working\", 'publication': 'Platformer', 'author': 'Casey Newton', 'url': 'https://www.platformer.news/p/why-labels-on-misinformation-arent', 'published': datetime.datetime(2020, 12, 14, 20, 0, 1), 'content_words': 65, 'score': 63743572801000000}, {'title': 'EHR interoperability: why it matters', 'publication': 'In Silico', 'author': 'Scott Xiao', 'url': 'https://insilico.substack.com/p/ehr-interoperability-why-it-matters', 'published': datetime.datetime(2020, 12, 14, 17, 44, 23), 'content_words': 1830, 'score': 63743564663000000}, {'title': '\"personal social networking\" is the working market definition, in the legal jargon.', 'publication': 'Stories by Tim Wu on Medium', 'author': 'Tim Wu', 'url': 'https://medium.com/@superwuster/personal-social-networking-is-the-working-market-definition-in-the-legal-jargon-663c4544aa25?source=rss-7d8da5c1a768------2', 'published': datetime.datetime(2020, 12, 14, 16, 20, 28), 'content_words': 12, 'score': 63743559628000000}, {'title': 'The IPO Market: Too Expensive for Sellers', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/the-ipo-market-too-expensive-for', 'published': datetime.datetime(2020, 12, 14, 10, 9, 25), 'content_words': 59, 'score': 63743537365000000}, {'title': 'Everybody Hates Facebook', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/everybody-hates-facecbook', 'published': datetime.datetime(2020, 12, 14, 9, 7, 21), 'content_words': 6286, 'score': 63743533641000000}, {'title': 'Everybody Hates Facebook', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/everybody-hates-facebook', 'published': datetime.datetime(2020, 12, 14, 8, 54, 46), 'content_words': 6299, 'score': 63743532886000000}, {'title': 'What comes after smartphones?', 'publication': 'Essays - Benedict Evans', 'author': 'Benedict Evans', 'url': 'https://www.ben-evans.com/benedictevans/2020/12/13/what-comes-after-smartphones', 'published': datetime.datetime(2020, 12, 13, 18, 38, 56), 'content_words': 1103, 'score': 63743481536000000}, {'title': 'The Ins-and-Outs of Cancer Care Navigators With Laura Stratte', 'publication': 'Out-Of-Pocket', 'author': 'Nikhil Krishnan', 'url': 'https://outofpocket.health/p/the-ins-and-outs-of-cancer-care-navigators', 'published': datetime.datetime(2020, 12, 13, 15, 12, 8), 'content_words': 3261, 'score': 63743469128000000}, {'title': \"Revisiting The Generalist's 2020 Predictions\", 'publication': 'The Generalist', 'author': 'MDA Gabriele', 'url': 'https://thegeneralist.substack.com/p/revisiting-the-generalists-2020-predictions', 'published': datetime.datetime(2020, 12, 13, 12, 31), 'content_words': 3390, 'score': 63743459460000000}, {'title': '\"WEEK 27\"', 'publication': 'Keeping Up With India', 'author': 'Vedica Kant', 'url': 'https://hind.substack.com/p/week-27', 'published': datetime.datetime(2020, 12, 13, 2, 11, 14), 'content_words': 1436, 'score': 63743422274000000}, {'title': 'Ideal sprint length, designer vs. PM roles, running PM team meetings, running post-mortems, best product/executive coaches, and much more', 'publication': \"Lenny's Newsletter\", 'author': 'Kiyani', 'url': 'https://www.lennyrachitsky.com/p/ideal-sprint-lengths-running-pm-team', 'published': datetime.datetime(2020, 12, 11, 11, 35, 52), 'content_words': 65, 'score': 63743283352000000}, {'title': 'Roblox and the Dispersal of Creativity', 'publication': 'Prof Galloway', 'author': 'Scott Galloway', 'url': 'https://www.profgalloway.com/roblox-and-the-dispersal-of-creativity/', 'published': datetime.datetime(2020, 12, 11, 10, 49, 10), 'content_words': 1695, 'score': 63743280550000000}, {'title': 'Sharing and Owning Standards', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/sharing-and-owning-standards', 'published': datetime.datetime(2020, 12, 11, 10, 45, 24), 'content_words': 3051, 'score': 63743280324000000}, {'title': 'CYP &#8211; Become a Startupy First Friend', 'publication': 'Check your Pulse', 'author': 'Sari', 'url': 'https://sariazout.substack.com/p/cyp-become-a-startupy-first-friend', 'published': datetime.datetime(2020, 12, 11, 8, 10, 36), 'content_words': 467, 'score': 63743271036000000}, {'title': '7 questions and answers about the Facebook antitrust case', 'publication': 'Platformer', 'author': 'Casey Newton', 'url': 'https://www.platformer.news/p/7-questions-and-answers-about-the', 'published': datetime.datetime(2020, 12, 10, 20, 0, 42), 'content_words': 65, 'score': 63743227242000000}, {'title': 'Three things the press keeps getting wrong about the Facebook antitrust case', 'publication': 'Stories by Tim Wu on Medium', 'author': 'Tim Wu', 'url': 'https://medium.com/@superwuster/three-things-the-press-keeps-getting-wrong-about-the-facebook-antitrust-case-1916a5f103d4?source=rss-7d8da5c1a768------2', 'published': datetime.datetime(2020, 12, 10, 13, 48, 15), 'content_words': 18, 'score': 63743204895000000}, {'title': 'Understanding Flag Carriers', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/understanding-flag-carriers', 'published': datetime.datetime(2020, 12, 10, 11, 33, 30), 'content_words': 67, 'score': 63743196810000000}, {'title': 'Outfit: Not Boring Investment Memo', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/outfit-not-boring-investment-memo', 'published': datetime.datetime(2020, 12, 10, 8, 57, 5), 'content_words': 4813, 'score': 63743187425000000}, {'title': 'Outfit: Not Boring Memo (Audio)', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/outfit-not-boring-memo-audio', 'published': datetime.datetime(2020, 12, 10, 8, 56, 26), 'content_words': 4800, 'score': 63743187386000000}, {'title': 'The FTC seeks to break up Facebook', 'publication': 'Platformer', 'author': 'Casey Newton', 'url': 'https://www.platformer.news/p/the-ftc-seeks-to-break-up-facebook', 'published': datetime.datetime(2020, 12, 9, 20, 0, 57), 'content_words': 2779, 'score': 63743140857000000}, {'title': 'Would community group buying work in your part of the world?', 'publication': 'Chinese Characteristics', 'author': 'Lillian Li', 'url': 'https://lillianli.substack.com/p/would-community-group-buying-work/comments', 'published': datetime.datetime(2020, 12, 9, 14, 21, 14), 'content_words': 138, 'score': 63743120474000000}, {'title': 'Bonus Material: Community Group Buying', 'publication': 'Chinese Characteristics', 'author': 'Lillian Li', 'url': 'https://lillianli.substack.com/p/bonus-material-community-group-buying', 'published': datetime.datetime(2020, 12, 9, 14, 3, 31), 'content_words': 70, 'score': 63743119411000000}, {'title': 'Six Meta-Narratives That Define How Americans Experience the News', 'publication': 'Stories by Tim Wu on Medium', 'author': 'Tim Wu', 'url': 'https://medium.com/@superwuster/six-meta-narratives-that-define-how-americans-experience-the-news-7fb5f1529ad6?source=rss-7d8da5c1a768------2', 'published': datetime.datetime(2020, 12, 9, 12, 19, 24), 'content_words': 12, 'score': 63743113164000000}, {'title': 'Slack-Salesforce and The Next $10B+ Software Acquisition', 'publication': 'next big thing', 'author': 'Nikhil Basu Trivedi', 'url': 'https://nbt.substack.com/p/slack-salesforce-and-the-next-10b', 'published': datetime.datetime(2020, 12, 9, 11, 47, 30), 'content_words': 1670, 'score': 63743111250000000}, {'title': 'CYP &#8211; Introducing startupy.world', 'publication': 'Check your Pulse', 'author': 'Sari', 'url': 'https://sariazout.substack.com/p/cyp-introducing-startupyworld', 'published': datetime.datetime(2020, 12, 9, 11, 29, 44), 'content_words': 843, 'score': 63743110184000000}, {'title': 'Affirm: The Morality of Money', 'publication': 'The Generalist', 'author': 'MDA Gabriele', 'url': 'https://thegeneralist.substack.com/p/affirm-the-morality-of-money', 'published': datetime.datetime(2020, 12, 9, 11, 21, 33), 'content_words': 8405, 'score': 63743109693000000}, {'title': 'The Three Stages of the Future of Work', 'publication': 'Digital Native', 'author': 'Rex Woodbury', 'url': 'https://digitalnative.substack.com/p/the-three-stages-of-the-future-of', 'published': datetime.datetime(2020, 12, 9, 10, 20, 3), 'content_words': 1902, 'score': 63743106003000000}, {'title': 'What If SPACs are a Shift, Not a Bubble?', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/what-if-spacs-are-a-shift-not-a-bubble', 'published': datetime.datetime(2020, 12, 9, 9, 52, 3), 'content_words': 69, 'score': 63743104323000000}, {'title': \"Apple and Google's contact tracing tragedy\", 'publication': 'Platformer', 'author': 'Casey Newton', 'url': 'https://www.platformer.news/p/apple-and-googles-contact-tracing', 'published': datetime.datetime(2020, 12, 8, 20, 0, 11), 'content_words': 62, 'score': 63743054411000000}, {'title': 'How Live-streaming Ecommerce Works in China', 'publication': 'Napkin Math', 'author': 'Yiren Lu and Grace Zhang', 'url': 'https://napkinmath.substack.com/p/how-live-streaming-ecommerce-works', 'published': datetime.datetime(2020, 12, 8, 13, 14, 49), 'content_words': 1394, 'score': 63743030089000000}, {'title': 'Generating buzz &#8211; Issue 55', 'publication': \"Lenny's Newsletter\", 'author': 'Lenny Rachitsky', 'url': 'https://www.lennyrachitsky.com/p/creating-buzz-at-launch', 'published': datetime.datetime(2020, 12, 8, 11, 29, 5), 'content_words': 55, 'score': 63743023745000000}, {'title': 'Winner-Take-Some Platforms', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/winner-take-some-platforms', 'published': datetime.datetime(2020, 12, 8, 9, 16, 41), 'content_words': 55, 'score': 63743015801000000}, {'title': 'Privacy Labels and Lookalike Audiences', 'publication': 'Stratechery by Ben Thompson', 'author': 'Ben Thompson', 'url': 'https://stratechery.com/2020/privacy-labels-and-lookalike-audiences/', 'published': datetime.datetime(2020, 12, 8, 6, 26, 45), 'content_words': 3784, 'score': 63743005605000000}, {'title': \"Four questions about Google's firing of Timnit Gebru\", 'publication': 'Platformer', 'author': 'Casey Newton', 'url': 'https://www.platformer.news/p/four-questions-about-googles-timnit', 'published': datetime.datetime(2020, 12, 7, 20, 0, 48), 'content_words': 55, 'score': 63742968048000000}, {'title': 'E-Commerce, Advertising and Perfect Price Discrimination', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/e-commerce-advertising-and-perfect', 'published': datetime.datetime(2020, 12, 7, 11, 16, 16), 'content_words': 60, 'score': 63742936576000000}, {'title': 'APIs All the Way Down (Audio)', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/apis-all-the-way-down-audio', 'published': datetime.datetime(2020, 12, 7, 9, 15, 26), 'content_words': 6385, 'score': 63742929326000000}, {'title': 'APIs All the Way Down', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/apis-all-the-way-down', 'published': datetime.datetime(2020, 12, 7, 8, 55, 35), 'content_words': 6398, 'score': 63742928135000000}, {'title': 'Nintendo: Infinite Games', 'publication': 'The Generalist', 'author': 'MDA Gabriele', 'url': 'https://thegeneralist.substack.com/p/nintendo-infinite-games', 'published': datetime.datetime(2020, 12, 6, 14, 18, 5), 'content_words': 3812, 'score': 63742861085000000}, {'title': 'Retail and community pharmacies are changing', 'publication': 'Out-Of-Pocket', 'author': 'Nikhil Krishnan', 'url': 'https://outofpocket.health/p/retail-and-community-pharmacies-are', 'published': datetime.datetime(2020, 12, 6, 12, 16, 13), 'content_words': 3022, 'score': 63742853773000000}, {'title': \"Fixing Audit Won't Fix Fraud: An Interview with Kris Bennatti of Bedrock AI\", 'publication': \"Alex Danco's Newsletter\", 'author': 'Alex Danco', 'url': 'https://danco.substack.com/p/fixing-audit-wont-fix-fraud-an-interview', 'published': datetime.datetime(2020, 12, 6, 10, 0, 52), 'content_words': 2709, 'score': 63742845652000000}, {'title': '\"WEEK 26\"', 'publication': 'Keeping Up With India', 'author': 'Anmol Maini', 'url': 'https://hind.substack.com/p/week-26', 'published': datetime.datetime(2020, 12, 5, 23, 54, 21), 'content_words': 1497, 'score': 63742809261000000}, {'title': 'Final Reminder: OOP Slack', 'publication': 'Out-Of-Pocket', 'author': 'Nikhil Krishnan', 'url': 'https://outofpocket.health/p/final-reminder-oop-slack', 'published': datetime.datetime(2020, 12, 4, 12, 12, 59), 'content_words': 1591, 'score': 63742680779000000}, {'title': 'Taiwan and Supply Chain Frenmity', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/taiwan-and-supply-chain-frenmity', 'published': datetime.datetime(2020, 12, 4, 12, 7, 5), 'content_words': 3329, 'score': 63742680425000000}, {'title': 'The Great Dispersion', 'publication': 'Prof Galloway', 'author': 'Scott Galloway', 'url': 'https://www.profgalloway.com/the-great-dispersion/', 'published': datetime.datetime(2020, 12, 4, 11, 46, 46), 'content_words': 1183, 'score': 63742679206000000}, {'title': 'SEO keywords, career ladders, backlog tools, copywriting, OnlyFans, AMA with Pete Kazanjy and much more', 'publication': \"Lenny's Newsletter\", 'author': 'Kiyani', 'url': 'https://www.lennyrachitsky.com/p/seo-keywords-career-ladders-backlog', 'published': datetime.datetime(2020, 12, 4, 11, 5, 21), 'content_words': 60, 'score': 63742676721000000}, {'title': 'A firing roils Google', 'publication': 'Platformer', 'author': 'Casey Newton', 'url': 'https://www.platformer.news/p/a-firing-roils-google', 'published': datetime.datetime(2020, 12, 3, 20, 0, 36), 'content_words': 61, 'score': 63742622436000000}, {'title': 'The hottest and least understood e-commerce model: Community Group Buying', 'publication': 'Chinese Characteristics', 'author': 'Lillian Li', 'url': 'https://lillianli.substack.com/p/the-hottest-and-least-understood', 'published': datetime.datetime(2020, 12, 3, 13, 39, 30), 'content_words': 2241, 'score': 63742599570000000}, {'title': 'DoorDash: The Value of Speed', 'publication': 'The Generalist', 'author': 'MDA Gabriele', 'url': 'https://thegeneralist.substack.com/p/doordash-the-value-of-speed', 'published': datetime.datetime(2020, 12, 3, 13, 21, 50), 'content_words': 8945, 'score': 63742598510000000}, {'title': 'The withering email that got an ethical AI researcher fired at Google', 'publication': 'Platformer', 'author': 'Casey Newton', 'url': 'https://www.platformer.news/p/the-withering-email-that-got-an-ethical', 'published': datetime.datetime(2020, 12, 3, 12, 38, 41), 'content_words': 1822, 'score': 63742595921000000}, {'title': 'Taiwan Semiconductor Manufacturing: A Successful Faith-Based Organization', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/taiwan-semiconductor-manufacturing', 'published': datetime.datetime(2020, 12, 3, 12, 11, 40), 'content_words': 53, 'score': 63742594300000000}]\n"
     ]
    }
   ],
   "source": [
    "from thinkpiecer import *\n",
    "\n",
    "# Let's try the real code\n",
    "# Load the real index and search it \n",
    "\n",
    "ix = load_index()\n",
    "\n",
    "print(get_recent_articles(ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count\n",
    "import re\n",
    "line = \" I am having a very -- nice @ 'day'.\"\n",
    "count = len(re.findall(r'\\w+', line))\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
