{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser, FuzzyTermPlugin\n",
    "from whoosh import highlight\n",
    "\n",
    "import calendar\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "NewsFeed = feedparser.parse(\"https://thegeneralist.substack.com/feed\")\n",
    "\n",
    "entry = NewsFeed.entries[0]\n",
    "print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entry['title'])\n",
    "print(entry['author'])\n",
    "print(entry['summary'])\n",
    "print(entry['link'])\n",
    "print(entry['published_parsed'])\n",
    "content = entry['content'][0]['value']\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn HTML into clean text for nice searching\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "tree = BeautifulSoup(content)\n",
    "\n",
    "# This validates and cleans HTML but Substack HTML is already fine\n",
    "# pretty = tree.prettify()\n",
    "# print(pretty)\n",
    "\n",
    "pure_text = tree.get_text(\"\\n\")\n",
    "print(pure_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, build a Whoosh database and search engine\n",
    "# https://whoosh.readthedocs.io/en/latest/quickstart.html#a-quick-introduction\n",
    "\n",
    "# Define schema (pretty simple)\n",
    "# Add documents from all top Substacks\n",
    "# Build a searcher\n",
    "# Show results\n",
    "# Highlight bits from the thinkpieces that match\n",
    "# Optional: add stemming for better searching\n",
    "# Later, add support for other newsletters with an RSS feed\n",
    "# Maybe, later, offer `more_like_this` so people can rabbit-hole in\n",
    "\n",
    "# Use case: I want to find all articles from my favorite writers that include the term\n",
    "# \"BNPL\" since I want to research \"Buy Now, Pay Later\" in fintech.\n",
    "# Or I want to find all thinkpieces that mentioned Google in the last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all RSSes to search\n",
    "\n",
    "# First, Substacks. Getting their RSS feeds is pretty simple/systematic.\n",
    "substack_domains = [\n",
    "    \"thegeneralist\",\n",
    "    \"danco\",\n",
    "    \"diff\",\n",
    "    \"nbt\",\n",
    "    \"platformer\",\n",
    "    \"notboring\",\n",
    "    \"sariazout\",\n",
    "    \"digitalnative\",\n",
    "    \"jamesonstartups\",\n",
    "    \"breakingsmart\",\n",
    "    \"artofgig\",\n",
    "    \"theskip\",\n",
    "    \"gwern\"\n",
    "]\n",
    "\n",
    "# Feeds are, e.g., https://thegeneralist.substack.com/feed\n",
    "substack_feeds = [\"https://{0}.substack.com/feed\".format(domain) for domain in substack_domains]\n",
    "\n",
    "# Now add some custom RSS feeds\n",
    "# Medium feeds are medium.com/feed/@user or medium.com/feed/publication\n",
    "custom_feeds = [\n",
    "    \"https://stratechery.com/feed/\",\n",
    "    \"https://www.profgalloway.com/feed\",\n",
    "    \"https://eugene-wei.squarespace.com/blog?format=rss\",\n",
    "    \"https://medium.com/feed/@superwuster\",\n",
    "    \"https://commoncog.com/blog/rss\",\n",
    "    \"https://www.lennyrachitsky.com/feed\",\n",
    "    \"https://medium.com/feed/bloated-mvp\",\n",
    "    \"https://daringfireball.net/feeds/main\",\n",
    "    \"https://wongmjane.com/api/feed/rss\",\n",
    "    \"https://fourweekmba.com/feed\",\n",
    "]\n",
    "\n",
    "# Unite all feeds into one\n",
    "all_feeds = substack_feeds + custom_feeds\n",
    "all_feeds\n",
    "\n",
    "# We'll read in the feeds from each of these. Get RSS feed at \n",
    "# https://thegeneralist.substack.com/feed\n",
    "\n",
    "# NewsFeed = feedparser.parse(\"https://thegeneralist.substack.com/feed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function to safely get an item from a dict.\n",
    "# If the key doesn't exist, just returns none\n",
    "def safe_get(obj, key):\n",
    "    if obj.has_key(key):\n",
    "        return obj[key]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows optional case-sensitive searches. all-lowercase is case insensitive, \n",
    "# any capital letters makes it case sensitive \n",
    "\n",
    "class CaseSensitivizer(analysis.Filter):\n",
    "    def __call__(self, tokens):\n",
    "        for t in tokens:\n",
    "            yield t\n",
    "            if t.mode == \"index\":\n",
    "               low = t.text.lower()\n",
    "               if low != t.text:\n",
    "                   t.text = low\n",
    "                   yield t\n",
    "\n",
    "ana = analysis.RegexTokenizer() | CaseSensitivizer()\n",
    "# [t.text for t in ana(\"The new SuperTurbo 5000\", mode=\"index\")]\n",
    "# [\"The\", \"the\", \"new\", \"SuperTurbo\", \"superturbo\", \"5000\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the database\n",
    "schema = Schema(\n",
    "    title=TEXT(stored=True),\n",
    "    author=TEXT(stored=True),\n",
    "    publication=TEXT(stored=True),\n",
    "    summary=TEXT(stored=True),\n",
    "    url=TEXT(stored=True),\n",
    "    published=DATETIME(stored=True, sortable=True),\n",
    "    content=TEXT(stored=True, analyzer=ana))\n",
    "index = create_in(\"whoosh_index2\", schema)\n",
    "writer = index.writer()\n",
    "\n",
    "# Read every item from our RSS feeds into there\n",
    "# Call writer.add_document() repeatedly for each item \n",
    "\n",
    "for feed_url in all_feeds:\n",
    "    print(feed_url)\n",
    "    news_feed = feedparser.parse(feed_url)\n",
    "    \n",
    "    # NOTE: we can only get the last few entries from this RSS feed.\n",
    "    # Substack doesn't seem to show anything older than the last 20.\n",
    "    # So we should build in a system to start caching these.\n",
    "    \n",
    "#     print(len(news_feed.entries))\n",
    "\n",
    "    for entry in news_feed.entries:\n",
    "        \n",
    "        # Get publication name. This is in the feed's `feed` field, along with other metadata\n",
    "        publication = None\n",
    "        metadata = safe_get(news_feed, 'feed')\n",
    "        if metadata is not None:\n",
    "            publication = safe_get(metadata, 'title')\n",
    "\n",
    "        # Clean up the date into a normal datetime\n",
    "        clean_datetime = datetime.fromtimestamp(calendar.timegm(entry['published_parsed']))\n",
    "        \n",
    "        # Most feeds put the main content in `content`,\n",
    "        # but a rare few like Eugene Wei put it in `summary`\n",
    "        # (in which case `content` is empty). With this logic, let's get a single `content` field.\n",
    "        body_text = None\n",
    "        # See if `content` exists\n",
    "        content_holder = safe_get(entry, 'content')\n",
    "        if content_holder is not None:\n",
    "            # We have content; fill it in\n",
    "            content_tree = BeautifulSoup(content_holder[0]['value'])\n",
    "            body_text = content_tree.get_text(\" \", strip=True)\n",
    "        else:\n",
    "            # No content provided. `summary` must hold all the text.\n",
    "            summary_tree = BeautifulSoup(safe_get(entry, 'summary'))\n",
    "            body_text = summary_tree.get_text(\" \", strip=True)\n",
    "\n",
    "        writer.add_document(\n",
    "            title=safe_get(entry, 'title'),\n",
    "            author=safe_get(entry, 'author'),\n",
    "            publication=publication,\n",
    "            summary=safe_get(entry, 'summary'),\n",
    "            url=safe_get(entry, 'link'),\n",
    "            published=clean_datetime,\n",
    "            content=body_text)\n",
    "\n",
    "print(\"DONE!\")\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, we're overriding the standard fragment formatter\n",
    "class BracketFormatter(highlight.Formatter):\n",
    "    \"\"\"Puts square brackets around the matched terms.\n",
    "    \"\"\"\n",
    "\n",
    "    def format_token(self, text, token, replace=False):\n",
    "        # Use the get_text function to get the text corresponding to the\n",
    "        # token\n",
    "        tokentext = highlight.get_text(text, token, replace)\n",
    "\n",
    "        # Return the text as you want it to appear in the highlighted\n",
    "        # string\n",
    "        return \"[[%s]]\" % tokentext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try searching\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "import whoosh.qparser as qparser\n",
    "\n",
    "search_term = \"Notion\"\n",
    "\n",
    "with index.searcher() as searcher:\n",
    "    parser = QueryParser(\"content\", index.schema)\n",
    "    # Allow fuzzy matching (EDIT: kinda screws things up)\n",
    "    # parser.add_plugin(FuzzyTermPlugin())\n",
    "    # Allow searching for entire phrases w/ single quotes, like 'microsoft teams'\n",
    "    parser.add_plugin(qparser.SingleQuotePlugin())\n",
    "    \n",
    "    query = parser.parse(search_term)\n",
    "    results = searcher.search(query, limit=None)\n",
    "    \n",
    "    # Highlighting settings\n",
    "    # This provides more context characters around the searched-for text\n",
    "    results.fragmenter.surround = 50\n",
    "    results.fragmenter.maxchars = 500\n",
    "    \n",
    "    # Surround matched tags with brackets\n",
    "    results.formatter = BracketFormatter()\n",
    "    \n",
    "    # Convert each Hit into a dict\n",
    "    def extract_hit_info(hit):\n",
    "        return {\n",
    "            'title': hit.get('title'),\n",
    "            'publication': hit.get('publication'),\n",
    "            'author': hit.get('author'),\n",
    "            'url': hit.get('url'),\n",
    "            'highlights': hit.highlights(\"content\", top=3),\n",
    "            'published': hit.get('published'),\n",
    "            'score': hit.score\n",
    "        }\n",
    "    \n",
    "    hit_list = [extract_hit_info(h) for h in results]\n",
    "    \n",
    "    print(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental\n",
    "\n",
    "news_feed = feedparser.parse(\"https://diff.substack.com/feed\")\n",
    "print(news_feed['feed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: read from the thinkpiecer module and use it here, instead of writing custom code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = {'a':5, 'b':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.get('c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the newest items.\n",
    "\n",
    "# Try searching\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "import whoosh.qparser as qparser\n",
    "from whoosh.qparser.dateparse import DateParserPlugin\n",
    "\n",
    "search_term = \"'-26 weeks to now'\"\n",
    "\n",
    "with index.searcher() as searcher:\n",
    "    parser = QueryParser(\"published\", index.schema)\n",
    "    # Allow fuzzy matching (EDIT: kinda screws things up)\n",
    "    # parser.add_plugin(FuzzyTermPlugin())\n",
    "    # Allow searching for entire phrases w/ single quotes, like 'microsoft teams'\n",
    "    parser.add_plugin(qparser.SingleQuotePlugin())\n",
    "    \n",
    "    # Add the DateParserPlugin to the parser\n",
    "    parser.add_plugin(DateParserPlugin())\n",
    "    \n",
    "    query = parser.parse(search_term)\n",
    "    results = searcher.search(query, limit=50, sortedby=\"published\", reverse=True)\n",
    "    \n",
    "    # Highlighting settings\n",
    "    # This provides more context characters around the searched-for text\n",
    "#     results.fragmenter.surround = 50\n",
    "#     results.fragmenter.maxchars = 500\n",
    "    \n",
    "    # Surround matched tags with brackets\n",
    "#     results.formatter = BracketFormatter()\n",
    "    \n",
    "    # Convert each Hit into a dict\n",
    "    def extract_hit_info(hit):\n",
    "        return {\n",
    "            'title': hit.get('title'),\n",
    "            'publication': hit.get('publication'),\n",
    "            'author': hit.get('author'),\n",
    "            'url': hit.get('url'),\n",
    "#             'highlights': hit.highlights(\"content\", top=3),\n",
    "            'published': hit.get('published'),\n",
    "            'score': hit.score\n",
    "        }\n",
    "    \n",
    "    hit_list = [extract_hit_info(h) for h in results]\n",
    "    \n",
    "    print(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 12, 13, 13, 14, 16, 18, 21, 23, 29, 35, 43, 51, 52, 53, 55, 55, 55, 55, 55, 55, 57, 57, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 61, 61, 62, 62, 63, 63, 63, 64, 64, 65, 65, 65, 65, 65, 65, 65, 67, 67, 67, 68, 69, 70, 70, 71, 72, 72, 73, 73, 79, 80, 118, 136, 138, 168, 204, 218, 252, 258, 297, 303, 323, 358, 368, 419, 445, 446, 467, 522, 525, 556, 571, 646, 654, 660, 665, 678, 683, 687, 695, 730, 738, 739, 779, 792, 793, 811, 843, 853, 868, 881, 889, 893, 895, 901, 924, 929, 938, 948, 956, 958, 1008, 1024, 1040, 1088, 1090, 1093, 1096, 1103, 1111, 1124, 1136, 1141, 1172, 1182, 1183, 1193, 1194, 1194, 1195, 1209, 1219, 1222, 1245, 1272, 1274, 1285, 1301, 1302, 1311, 1315, 1325, 1355, 1359, 1369, 1373, 1373, 1381, 1383, 1394, 1394, 1395, 1399, 1413, 1419, 1422, 1436, 1466, 1483, 1490, 1497, 1511, 1514, 1525, 1546, 1554, 1557, 1558, 1571, 1571, 1573, 1589, 1591, 1594, 1646, 1653, 1659, 1668, 1670, 1671, 1671, 1678, 1683, 1695, 1704, 1711, 1712, 1718, 1720, 1745, 1754, 1777, 1796, 1806, 1812, 1822, 1830, 1841, 1859, 1863, 1868, 1876, 1889, 1893, 1898, 1902, 1903, 1928, 1929, 1945, 1970, 1977, 1981, 1986, 1986, 2000, 2001, 2001, 2025, 2035, 2064, 2073, 2077, 2080, 2087, 2093, 2098, 2101, 2124, 2130, 2138, 2150, 2182, 2188, 2190, 2190, 2191, 2194, 2198, 2211, 2214, 2233, 2241, 2262, 2262, 2267, 2271, 2280, 2298, 2305, 2308, 2320, 2341, 2341, 2343, 2378, 2391, 2392, 2400, 2401, 2405, 2448, 2468, 2476, 2479, 2484, 2496, 2500, 2501, 2509, 2512, 2514, 2521, 2527, 2546, 2576, 2595, 2599, 2616, 2620, 2664, 2672, 2678, 2680, 2686, 2691, 2693, 2709, 2736, 2762, 2779, 2799, 2800, 2802, 2856, 2874, 2889, 2940, 2952, 2957, 2963, 2979, 3022, 3022, 3043, 3051, 3055, 3096, 3129, 3164, 3224, 3258, 3260, 3261, 3288, 3292, 3303, 3319, 3329, 3390, 3396, 3418, 3631, 3659, 3665, 3698, 3732, 3777, 3784, 3812, 3865, 4055, 4317, 4491, 4509, 4532, 4662, 4786, 4800, 4801, 4813, 4924, 5274, 5290, 5448, 5456, 5558, 5634, 5747, 5759, 6145, 6179, 6203, 6214, 6268, 6286, 6299, 6385, 6398, 6565, 6657, 6894, 6917, 7229, 7526, 7540, 8405, 8945, 9221]\n",
      "[{'title': 'Tune in today at 2 PM EST / 11 PST', 'publication': \"Alex Danco's Newsletter\", 'author': 'Alex Danco', 'url': 'https://danco.substack.com/p/tune-in-today-at-2-pm-est-11-pst', 'published': datetime.datetime(2020, 11, 1, 10, 1, 51), 'content_words': 168, 'score': 63739821711000000}, {'title': 'Intermission', 'publication': 'Chinese Characteristics', 'author': 'Lillian Li', 'url': 'https://lillianli.substack.com/p/intermission', 'published': datetime.datetime(2020, 10, 28, 13, 19, 43), 'content_words': 204, 'score': 63739487983000000}, {'title': '&#129521; Google Workspace and owning the \"stack\"', 'publication': \"Parth, Adi, and Neel's Product Insights\", 'author': 'Parth, Adi, and Neel', 'url': 'https://productinsights.substack.com/p/google-workspace-and-owning-the-stack', 'published': datetime.datetime(2020, 10, 19, 20, 2, 37), 'content_words': 218, 'score': 63738734557000000}]\n"
     ]
    }
   ],
   "source": [
    "from thinkpiecer import *\n",
    "\n",
    "# Let's try the real code\n",
    "# Load the real index and search it \n",
    "\n",
    "ix = load_index()\n",
    "\n",
    "# Figure out the distribution of word counts. My theory is that there are a lot of\n",
    "# low-quality, \"preview-only / paywalled\" pieces contained here and they should be filtered out.\n",
    "# Is there a breakpoint from bad to good?\n",
    "recs = get_recent_articles(ix)\n",
    "wcs = [r['content_words'] for r in recs]\n",
    "print(sorted(wcs))\n",
    "\n",
    "midquals = [r for r in recs if r['content_words'] > 150 and r['content_words'] < 250]\n",
    "print(midquals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count\n",
    "import re\n",
    "line = \" I am having a very -- nice @ 'day'.\"\n",
    "count = len(re.findall(r'\\w+', line))\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
