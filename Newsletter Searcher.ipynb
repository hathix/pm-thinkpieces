{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser, FuzzyTermPlugin\n",
    "from whoosh import highlight\n",
    "\n",
    "import calendar\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "NewsFeed = feedparser.parse(\"https://thegeneralist.substack.com/feed\")\n",
    "\n",
    "entry = NewsFeed.entries[0]\n",
    "print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entry['title'])\n",
    "print(entry['author'])\n",
    "print(entry['summary'])\n",
    "print(entry['link'])\n",
    "print(entry['published_parsed'])\n",
    "content = entry['content'][0]['value']\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn HTML into clean text for nice searching\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "tree = BeautifulSoup(content)\n",
    "\n",
    "# This validates and cleans HTML but Substack HTML is already fine\n",
    "# pretty = tree.prettify()\n",
    "# print(pretty)\n",
    "\n",
    "pure_text = tree.get_text(\"\\n\")\n",
    "print(pure_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, build a Whoosh database and search engine\n",
    "# https://whoosh.readthedocs.io/en/latest/quickstart.html#a-quick-introduction\n",
    "\n",
    "# Define schema (pretty simple)\n",
    "# Add documents from all top Substacks\n",
    "# Build a searcher\n",
    "# Show results\n",
    "# Highlight bits from the thinkpieces that match\n",
    "# Optional: add stemming for better searching\n",
    "# Later, add support for other newsletters with an RSS feed\n",
    "# Maybe, later, offer `more_like_this` so people can rabbit-hole in\n",
    "\n",
    "# Use case: I want to find all articles from my favorite writers that include the term\n",
    "# \"BNPL\" since I want to research \"Buy Now, Pay Later\" in fintech.\n",
    "# Or I want to find all thinkpieces that mentioned Google in the last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all RSSes to search\n",
    "\n",
    "# First, Substacks. Getting their RSS feeds is pretty simple/systematic.\n",
    "substack_domains = [\n",
    "    \"thegeneralist\",\n",
    "    \"danco\",\n",
    "    \"diff\",\n",
    "    \"nbt\",\n",
    "    \"platformer\",\n",
    "    \"notboring\",\n",
    "    \"sariazout\",\n",
    "    \"digitalnative\",\n",
    "    \"jamesonstartups\",\n",
    "    \"breakingsmart\",\n",
    "    \"artofgig\",\n",
    "    \"theskip\",\n",
    "    \"gwern\"\n",
    "]\n",
    "\n",
    "# Feeds are, e.g., https://thegeneralist.substack.com/feed\n",
    "substack_feeds = [\"https://{0}.substack.com/feed\".format(domain) for domain in substack_domains]\n",
    "\n",
    "# Now add some custom RSS feeds\n",
    "# Medium feeds are medium.com/feed/@user or medium.com/feed/publication\n",
    "custom_feeds = [\n",
    "    \"https://stratechery.com/feed/\",\n",
    "    \"https://www.profgalloway.com/feed\",\n",
    "    \"https://eugene-wei.squarespace.com/blog?format=rss\",\n",
    "    \"https://medium.com/feed/@superwuster\",\n",
    "    \"https://commoncog.com/blog/rss\",\n",
    "    \"https://www.lennyrachitsky.com/feed\",\n",
    "    \"https://medium.com/feed/bloated-mvp\",\n",
    "    \"https://daringfireball.net/feeds/main\",\n",
    "    \"https://wongmjane.com/api/feed/rss\",\n",
    "    \"https://fourweekmba.com/feed\",\n",
    "]\n",
    "\n",
    "# Unite all feeds into one\n",
    "all_feeds = substack_feeds + custom_feeds\n",
    "all_feeds\n",
    "\n",
    "# We'll read in the feeds from each of these. Get RSS feed at \n",
    "# https://thegeneralist.substack.com/feed\n",
    "\n",
    "# NewsFeed = feedparser.parse(\"https://thegeneralist.substack.com/feed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function to safely get an item from a dict.\n",
    "# If the key doesn't exist, just returns none\n",
    "def safe_get(obj, key):\n",
    "    if obj.has_key(key):\n",
    "        return obj[key]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows optional case-sensitive searches. all-lowercase is case insensitive, \n",
    "# any capital letters makes it case sensitive \n",
    "\n",
    "class CaseSensitivizer(analysis.Filter):\n",
    "    def __call__(self, tokens):\n",
    "        for t in tokens:\n",
    "            yield t\n",
    "            if t.mode == \"index\":\n",
    "               low = t.text.lower()\n",
    "               if low != t.text:\n",
    "                   t.text = low\n",
    "                   yield t\n",
    "\n",
    "ana = analysis.RegexTokenizer() | CaseSensitivizer()\n",
    "# [t.text for t in ana(\"The new SuperTurbo 5000\", mode=\"index\")]\n",
    "# [\"The\", \"the\", \"new\", \"SuperTurbo\", \"superturbo\", \"5000\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the database\n",
    "schema = Schema(\n",
    "    title=TEXT(stored=True),\n",
    "    author=TEXT(stored=True),\n",
    "    publication=TEXT(stored=True),\n",
    "    summary=TEXT(stored=True),\n",
    "    url=TEXT(stored=True),\n",
    "    published=DATETIME(stored=True, sortable=True),\n",
    "    content=TEXT(stored=True, analyzer=ana))\n",
    "index = create_in(\"whoosh_index2\", schema)\n",
    "writer = index.writer()\n",
    "\n",
    "# Read every item from our RSS feeds into there\n",
    "# Call writer.add_document() repeatedly for each item \n",
    "\n",
    "for feed_url in all_feeds:\n",
    "    print(feed_url)\n",
    "    news_feed = feedparser.parse(feed_url)\n",
    "    \n",
    "    # NOTE: we can only get the last few entries from this RSS feed.\n",
    "    # Substack doesn't seem to show anything older than the last 20.\n",
    "    # So we should build in a system to start caching these.\n",
    "    \n",
    "#     print(len(news_feed.entries))\n",
    "\n",
    "    for entry in news_feed.entries:\n",
    "        \n",
    "        # Get publication name. This is in the feed's `feed` field, along with other metadata\n",
    "        publication = None\n",
    "        metadata = safe_get(news_feed, 'feed')\n",
    "        if metadata is not None:\n",
    "            publication = safe_get(metadata, 'title')\n",
    "\n",
    "        # Clean up the date into a normal datetime\n",
    "        clean_datetime = datetime.fromtimestamp(calendar.timegm(entry['published_parsed']))\n",
    "        \n",
    "        # Most feeds put the main content in `content`,\n",
    "        # but a rare few like Eugene Wei put it in `summary`\n",
    "        # (in which case `content` is empty). With this logic, let's get a single `content` field.\n",
    "        body_text = None\n",
    "        # See if `content` exists\n",
    "        content_holder = safe_get(entry, 'content')\n",
    "        if content_holder is not None:\n",
    "            # We have content; fill it in\n",
    "            content_tree = BeautifulSoup(content_holder[0]['value'])\n",
    "            body_text = content_tree.get_text(\" \", strip=True)\n",
    "        else:\n",
    "            # No content provided. `summary` must hold all the text.\n",
    "            summary_tree = BeautifulSoup(safe_get(entry, 'summary'))\n",
    "            body_text = summary_tree.get_text(\" \", strip=True)\n",
    "\n",
    "        writer.add_document(\n",
    "            title=safe_get(entry, 'title'),\n",
    "            author=safe_get(entry, 'author'),\n",
    "            publication=publication,\n",
    "            summary=safe_get(entry, 'summary'),\n",
    "            url=safe_get(entry, 'link'),\n",
    "            published=clean_datetime,\n",
    "            content=body_text)\n",
    "\n",
    "print(\"DONE!\")\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, we're overriding the standard fragment formatter\n",
    "class BracketFormatter(highlight.Formatter):\n",
    "    \"\"\"Puts square brackets around the matched terms.\n",
    "    \"\"\"\n",
    "\n",
    "    def format_token(self, text, token, replace=False):\n",
    "        # Use the get_text function to get the text corresponding to the\n",
    "        # token\n",
    "        tokentext = highlight.get_text(text, token, replace)\n",
    "\n",
    "        # Return the text as you want it to appear in the highlighted\n",
    "        # string\n",
    "        return \"[[%s]]\" % tokentext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try searching\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "import whoosh.qparser as qparser\n",
    "\n",
    "search_term = \"Notion\"\n",
    "\n",
    "with index.searcher() as searcher:\n",
    "    parser = QueryParser(\"content\", index.schema)\n",
    "    # Allow fuzzy matching (EDIT: kinda screws things up)\n",
    "    # parser.add_plugin(FuzzyTermPlugin())\n",
    "    # Allow searching for entire phrases w/ single quotes, like 'microsoft teams'\n",
    "    parser.add_plugin(qparser.SingleQuotePlugin())\n",
    "    \n",
    "    query = parser.parse(search_term)\n",
    "    results = searcher.search(query, limit=None)\n",
    "    \n",
    "    # Highlighting settings\n",
    "    # This provides more context characters around the searched-for text\n",
    "    results.fragmenter.surround = 50\n",
    "    results.fragmenter.maxchars = 500\n",
    "    \n",
    "    # Surround matched tags with brackets\n",
    "    results.formatter = BracketFormatter()\n",
    "    \n",
    "    # Convert each Hit into a dict\n",
    "    def extract_hit_info(hit):\n",
    "        return {\n",
    "            'title': hit.get('title'),\n",
    "            'publication': hit.get('publication'),\n",
    "            'author': hit.get('author'),\n",
    "            'url': hit.get('url'),\n",
    "            'highlights': hit.highlights(\"content\", top=3),\n",
    "            'published': hit.get('published'),\n",
    "            'score': hit.score\n",
    "        }\n",
    "    \n",
    "    hit_list = [extract_hit_info(h) for h in results]\n",
    "    \n",
    "    print(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental\n",
    "\n",
    "news_feed = feedparser.parse(\"https://diff.substack.com/feed\")\n",
    "print(news_feed['feed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: read from the thinkpiecer module and use it here, instead of writing custom code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = {'a':5, 'b':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.get('c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the newest items.\n",
    "\n",
    "# Try searching\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "import whoosh.qparser as qparser\n",
    "from whoosh.qparser.dateparse import DateParserPlugin\n",
    "\n",
    "search_term = \"'-26 weeks to now'\"\n",
    "\n",
    "with index.searcher() as searcher:\n",
    "    parser = QueryParser(\"published\", index.schema)\n",
    "    # Allow fuzzy matching (EDIT: kinda screws things up)\n",
    "    # parser.add_plugin(FuzzyTermPlugin())\n",
    "    # Allow searching for entire phrases w/ single quotes, like 'microsoft teams'\n",
    "    parser.add_plugin(qparser.SingleQuotePlugin())\n",
    "    \n",
    "    # Add the DateParserPlugin to the parser\n",
    "    parser.add_plugin(DateParserPlugin())\n",
    "    \n",
    "    query = parser.parse(search_term)\n",
    "    results = searcher.search(query, limit=50, sortedby=\"published\", reverse=True)\n",
    "    \n",
    "    # Highlighting settings\n",
    "    # This provides more context characters around the searched-for text\n",
    "#     results.fragmenter.surround = 50\n",
    "#     results.fragmenter.maxchars = 500\n",
    "    \n",
    "    # Surround matched tags with brackets\n",
    "#     results.formatter = BracketFormatter()\n",
    "    \n",
    "    # Convert each Hit into a dict\n",
    "    def extract_hit_info(hit):\n",
    "        return {\n",
    "            'title': hit.get('title'),\n",
    "            'publication': hit.get('publication'),\n",
    "            'author': hit.get('author'),\n",
    "            'url': hit.get('url'),\n",
    "#             'highlights': hit.highlights(\"content\", top=3),\n",
    "            'published': hit.get('published'),\n",
    "            'score': hit.score\n",
    "        }\n",
    "    \n",
    "    hit_list = [extract_hit_info(h) for h in results]\n",
    "    \n",
    "    print(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'EHR interoperability: why it matters', 'publication': 'In Silico', 'author': 'Scott Xiao', 'url': 'https://insilico.substack.com/p/ehr-interoperability-why-it-matters', 'published': datetime.datetime(2020, 12, 14, 17, 44, 23), 'score': 63743564663000000}, {'title': 'Everybody Hates Facebook', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/everybody-hates-facecbook', 'published': datetime.datetime(2020, 12, 14, 9, 7, 21), 'score': 63743533641000000}, {'title': 'Everybody Hates Facebook', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/everybody-hates-facebook', 'published': datetime.datetime(2020, 12, 14, 8, 54, 46), 'score': 63743532886000000}, {'title': 'What comes after smartphones?', 'publication': 'Essays - Benedict Evans', 'author': 'Benedict Evans', 'url': 'https://www.ben-evans.com/benedictevans/2020/12/13/what-comes-after-smartphones', 'published': datetime.datetime(2020, 12, 13, 18, 38, 56), 'score': 63743481536000000}, {'title': 'The Ins-and-Outs of Cancer Care Navigators With Laura Stratte', 'publication': 'Out-Of-Pocket', 'author': 'Nikhil Krishnan', 'url': 'https://outofpocket.health/p/the-ins-and-outs-of-cancer-care-navigators', 'published': datetime.datetime(2020, 12, 13, 15, 12, 8), 'score': 63743469128000000}, {'title': \"Revisiting The Generalist's 2020 Predictions\", 'publication': 'The Generalist', 'author': 'MDA Gabriele', 'url': 'https://thegeneralist.substack.com/p/revisiting-the-generalists-2020-predictions', 'published': datetime.datetime(2020, 12, 13, 12, 31), 'score': 63743459460000000}, {'title': '\"WEEK 27\"', 'publication': 'Keeping Up With India', 'author': 'Vedica Kant', 'url': 'https://hind.substack.com/p/week-27', 'published': datetime.datetime(2020, 12, 13, 2, 11, 14), 'score': 63743422274000000}, {'title': 'After a long wait, fully driverless is finally arriving', 'publication': 'Let Me Level 5 With You', 'author': 'Oliver Cameron', 'url': 'https://olivercameron.substack.com/p/after-a-long-wait-fully-driverless', 'published': datetime.datetime(2020, 12, 11, 16, 10, 22), 'score': 63743299822000000}, {'title': 'Roblox and the Dispersal of Creativity', 'publication': 'Prof Galloway', 'author': 'Scott Galloway', 'url': 'https://www.profgalloway.com/roblox-and-the-dispersal-of-creativity/', 'published': datetime.datetime(2020, 12, 11, 10, 49, 10), 'score': 63743280550000000}, {'title': 'Sharing and Owning Standards', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/sharing-and-owning-standards', 'published': datetime.datetime(2020, 12, 11, 10, 45, 24), 'score': 63743280324000000}, {'title': 'CYP &#8211; Become a Startupy First Friend', 'publication': 'Check your Pulse', 'author': 'Sari', 'url': 'https://sariazout.substack.com/p/cyp-become-a-startupy-first-friend', 'published': datetime.datetime(2020, 12, 11, 8, 10, 36), 'score': 63743271036000000}, {'title': 'Outfit: Not Boring Investment Memo', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/outfit-not-boring-investment-memo', 'published': datetime.datetime(2020, 12, 10, 8, 57, 5), 'score': 63743187425000000}, {'title': 'Outfit: Not Boring Memo (Audio)', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/outfit-not-boring-memo-audio', 'published': datetime.datetime(2020, 12, 10, 8, 56, 26), 'score': 63743187386000000}, {'title': 'The FTC seeks to break up Facebook', 'publication': 'Platformer', 'author': 'Casey Newton', 'url': 'https://www.platformer.news/p/the-ftc-seeks-to-break-up-facebook', 'published': datetime.datetime(2020, 12, 9, 20, 0, 57), 'score': 63743140857000000}, {'title': 'Slack-Salesforce and The Next $10B+ Software Acquisition', 'publication': 'next big thing', 'author': 'Nikhil Basu Trivedi', 'url': 'https://nbt.substack.com/p/slack-salesforce-and-the-next-10b', 'published': datetime.datetime(2020, 12, 9, 11, 47, 30), 'score': 63743111250000000}, {'title': 'CYP &#8211; Introducing startupy.world', 'publication': 'Check your Pulse', 'author': 'Sari', 'url': 'https://sariazout.substack.com/p/cyp-introducing-startupyworld', 'published': datetime.datetime(2020, 12, 9, 11, 29, 44), 'score': 63743110184000000}, {'title': 'Affirm: The Morality of Money', 'publication': 'The Generalist', 'author': 'MDA Gabriele', 'url': 'https://thegeneralist.substack.com/p/affirm-the-morality-of-money', 'published': datetime.datetime(2020, 12, 9, 11, 21, 33), 'score': 63743109693000000}, {'title': 'The Three Stages of the Future of Work', 'publication': 'Digital Native', 'author': 'Rex Woodbury', 'url': 'https://digitalnative.substack.com/p/the-three-stages-of-the-future-of', 'published': datetime.datetime(2020, 12, 9, 10, 20, 3), 'score': 63743106003000000}, {'title': 'How Live-streaming Ecommerce Works in China', 'publication': 'Napkin Math', 'author': 'Yiren Lu and Grace Zhang', 'url': 'https://napkinmath.substack.com/p/how-live-streaming-ecommerce-works', 'published': datetime.datetime(2020, 12, 8, 13, 14, 49), 'score': 63743030089000000}, {'title': 'Privacy Labels and Lookalike Audiences', 'publication': 'Stratechery by Ben Thompson', 'author': 'Ben Thompson', 'url': 'https://stratechery.com/2020/privacy-labels-and-lookalike-audiences/', 'published': datetime.datetime(2020, 12, 8, 6, 26, 45), 'score': 63743005605000000}, {'title': 'APIs All the Way Down (Audio)', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/apis-all-the-way-down-audio', 'published': datetime.datetime(2020, 12, 7, 9, 15, 26), 'score': 63742929326000000}, {'title': 'APIs All the Way Down', 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/apis-all-the-way-down', 'published': datetime.datetime(2020, 12, 7, 8, 55, 35), 'score': 63742928135000000}, {'title': '&#127829; DoorDash IPO - Nailing Food Delivery', 'publication': 'Overlooked by Alexandre Dewez', 'author': 'Alexandre Dewez', 'url': 'https://alexandre.substack.com/p/-doordash-ipo-nailing-food-delivery', 'published': datetime.datetime(2020, 12, 7, 6, 6, 49), 'score': 63742918009000000}, {'title': 'Nintendo: Infinite Games', 'publication': 'The Generalist', 'author': 'MDA Gabriele', 'url': 'https://thegeneralist.substack.com/p/nintendo-infinite-games', 'published': datetime.datetime(2020, 12, 6, 14, 18, 5), 'score': 63742861085000000}, {'title': 'Retail and community pharmacies are changing', 'publication': 'Out-Of-Pocket', 'author': 'Nikhil Krishnan', 'url': 'https://outofpocket.health/p/retail-and-community-pharmacies-are', 'published': datetime.datetime(2020, 12, 6, 12, 16, 13), 'score': 63742853773000000}, {'title': \"Fixing Audit Won't Fix Fraud: An Interview with Kris Bennatti of Bedrock AI\", 'publication': \"Alex Danco's Newsletter\", 'author': 'Alex Danco', 'url': 'https://danco.substack.com/p/fixing-audit-wont-fix-fraud-an-interview', 'published': datetime.datetime(2020, 12, 6, 10, 0, 52), 'score': 63742845652000000}, {'title': '\"WEEK 26\"', 'publication': 'Keeping Up With India', 'author': 'Anmol Maini', 'url': 'https://hind.substack.com/p/week-26', 'published': datetime.datetime(2020, 12, 5, 23, 54, 21), 'score': 63742809261000000}, {'title': 'Final Reminder: OOP Slack', 'publication': 'Out-Of-Pocket', 'author': 'Nikhil Krishnan', 'url': 'https://outofpocket.health/p/final-reminder-oop-slack', 'published': datetime.datetime(2020, 12, 4, 12, 12, 59), 'score': 63742680779000000}, {'title': 'Taiwan and Supply Chain Frenmity', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/taiwan-and-supply-chain-frenmity', 'published': datetime.datetime(2020, 12, 4, 12, 7, 5), 'score': 63742680425000000}, {'title': 'The Great Dispersion', 'publication': 'Prof Galloway', 'author': 'Scott Galloway', 'url': 'https://www.profgalloway.com/the-great-dispersion/', 'published': datetime.datetime(2020, 12, 4, 11, 46, 46), 'score': 63742679206000000}, {'title': 'The hottest and least understood e-commerce model: Community Group Buying', 'publication': 'Chinese Characteristics', 'author': 'Lillian Li', 'url': 'https://lillianli.substack.com/p/the-hottest-and-least-understood', 'published': datetime.datetime(2020, 12, 3, 13, 39, 30), 'score': 63742599570000000}, {'title': 'DoorDash: The Value of Speed', 'publication': 'The Generalist', 'author': 'MDA Gabriele', 'url': 'https://thegeneralist.substack.com/p/doordash-the-value-of-speed', 'published': datetime.datetime(2020, 12, 3, 13, 21, 50), 'score': 63742598510000000}, {'title': 'The withering email that got an ethical AI researcher fired at Google', 'publication': 'Platformer', 'author': 'Casey Newton', 'url': 'https://www.platformer.news/p/the-withering-email-that-got-an-ethical', 'published': datetime.datetime(2020, 12, 3, 12, 38, 41), 'score': 63742595921000000}, {'title': 'Stripe: Platform of Platforms', 'publication': 'Stratechery by Ben Thompson', 'author': 'Ben Thompson', 'url': 'https://stratechery.com/2020/stripe-platform-of-platforms/', 'published': datetime.datetime(2020, 12, 3, 10, 0, 11), 'score': 63742586411000000}, {'title': \"The regulator's puzzle\", 'publication': 'Essays - Benedict Evans', 'author': 'Benedict Evans', 'url': 'https://www.ben-evans.com/benedictevans/2020/12/03/the-regulators-puzzle', 'published': datetime.datetime(2020, 12, 3, 8, 5, 40), 'score': 63742579540000000}, {'title': 'How Microsoft crushed Slack', 'publication': 'Platformer', 'author': 'Casey Newton', 'url': 'https://www.platformer.news/p/how-microsoft-crushed-slack', 'published': datetime.datetime(2020, 12, 2, 20, 0, 40), 'score': 63742536040000000}, {'title': 'Venture Partnerships vs. Solo Capitalists', 'publication': 'next big thing', 'author': 'Nikhil Basu Trivedi', 'url': 'https://nbt.substack.com/p/venture-partnerships-vs-solo-capitalists', 'published': datetime.datetime(2020, 12, 2, 10, 5, 29), 'score': 63742500329000000}, {'title': 'Virtual Worlds and Virtual Economies', 'publication': 'Digital Native', 'author': 'Rex Woodbury', 'url': 'https://digitalnative.substack.com/p/the-startups-building-virtual-worlds', 'published': datetime.datetime(2020, 12, 2, 8, 44, 6), 'score': 63742495446000000}, {'title': 'Amazon Pharmacy: an obvious first step', 'publication': 'In Silico', 'author': 'Scott Xiao', 'url': 'https://insilico.substack.com/p/amazon-pharmacy-an-obvious-first', 'published': datetime.datetime(2020, 11, 30, 21, 35, 50), 'score': 63742368950000000}, {'title': 'Should you invest in Wish? An S-1 Analysis', 'publication': 'Napkin Math', 'author': 'Adam Keesling', 'url': 'https://napkinmath.substack.com/p/should-you-invest-in-wish-an-s-1', 'published': datetime.datetime(2020, 11, 30, 20, 7, 26), 'score': 63742363646000000}, {'title': '&#129340;&#8205;&#9792;&#65039; Collectives in the Creative and Business Worlds', 'publication': 'Overlooked by Alexandre Dewez', 'author': 'Alexandre Dewez', 'url': 'https://alexandre.substack.com/p/-collectives-in-the-creative-and', 'published': datetime.datetime(2020, 11, 30, 13, 46), 'score': 63742340760000000}, {'title': 'Airbnb: The Disaster Artist', 'publication': 'The Generalist', 'author': 'MDA Gabriele', 'url': 'https://thegeneralist.substack.com/p/airbnb-the-disaster-artist', 'published': datetime.datetime(2020, 11, 30, 13, 37, 24), 'score': 63742340244000000}, {'title': 'Five Lessons From Dave Chappelle', 'publication': 'Stratechery by Ben Thompson', 'author': 'Ben Thompson', 'url': 'https://stratechery.com/2020/five-lessons-from-dave-chappelle/', 'published': datetime.datetime(2020, 11, 30, 9, 48, 42), 'score': 63742326522000000}, {'title': \"We're Never Going Back (Audio)\", 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/were-never-going-back-audio', 'published': datetime.datetime(2020, 11, 30, 9, 20, 44), 'score': 63742324844000000}, {'title': \"We're Never Going Back\", 'publication': 'Not Boring by Packy McCormick', 'author': 'Packy McCormick', 'url': 'https://notboring.substack.com/p/were-never-going-back', 'published': datetime.datetime(2020, 11, 30, 8, 54, 14), 'score': 63742323254000000}, {'title': 'Reminder: OOP Slack Application', 'publication': 'Out-Of-Pocket', 'author': 'Nikhil Krishnan', 'url': 'https://outofpocket.health/p/reminder-oop-slack-application', 'published': datetime.datetime(2020, 11, 29, 21, 22, 23), 'score': 63742281743000000}, {'title': 'BFCM', 'publication': \"Alex Danco's Newsletter\", 'author': 'Alex Danco', 'url': 'https://danco.substack.com/p/bfcm', 'published': datetime.datetime(2020, 11, 29, 10, 0, 51), 'score': 63742240851000000}, {'title': '\"WEEK 25\"', 'publication': 'Keeping Up With India', 'author': 'Vedica Kant', 'url': 'https://hind.substack.com/p/week-25', 'published': datetime.datetime(2020, 11, 29, 2, 44, 41), 'score': 63742214681000000}, {'title': 'Just One Thing or Every Single Thing?', 'publication': 'The Diff', 'author': 'Byrne Hobart', 'url': 'https://diff.substack.com/p/just-one-thing-or-every-single-thing', 'published': datetime.datetime(2020, 11, 27, 10, 53, 32), 'score': 63742071212000000}, {'title': 'More Kuaishou, Less TikTok', 'publication': 'Emerging', 'author': 'Pondering Durian', 'url': 'https://emerging.substack.com/p/more-kuaishou-less-tiktok', 'published': datetime.datetime(2020, 11, 26, 19, 0, 48), 'score': 63742014048000000}]\n"
     ]
    }
   ],
   "source": [
    "from thinkpiecer import *\n",
    "\n",
    "# Let's try the real code\n",
    "# Load the real index and search it \n",
    "\n",
    "ix = load_index()\n",
    "\n",
    "# Figure out the distribution of word counts. My theory is that there are a lot of\n",
    "# low-quality, \"preview-only / paywalled\" pieces contained here and they should be filtered out.\n",
    "# Is there a breakpoint from bad to good?\n",
    "recs = get_recent_articles(ix)\n",
    "print(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "calendar.timegm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinkpiecer\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from whoosh import index\n",
    "from whoosh.fields import *\n",
    "from whoosh.writing import AsyncWriter\n",
    "from whoosh.qparser import QueryParser\n",
    "import whoosh.qparser as qparser\n",
    "from whoosh.qparser.dateparse import DateParserPlugin\n",
    "from whoosh import highlight\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "import os, os.path\n",
    "\n",
    "# Local modules\n",
    "import utilities\n",
    "from utilities import safe_get\n",
    "import feeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try searching for recent high quality articles\n",
    "ix = thinkpiecer.load_index()\n",
    "\n",
    "with ix.searcher() as searcher:\n",
    "    # Search for the newest items.\n",
    "    # We have to search for SOMETHING so let's filter out all the\n",
    "    # articles that are too short. Then we can sort by date desc.\n",
    "    parser = QueryParser(\"content_word_count\", ix.schema)\n",
    "    \n",
    "    # Add a parser to do numerical comparisons\n",
    "    # This is the \"greater than, less than\" plugin\n",
    "    parser.add_plugin(qparser.GtLtPlugin())\n",
    "    search_term = \"content_word_count:>=250\"\n",
    "    query = parser.parse(search_term)\n",
    "    \n",
    "\n",
    "    # We have to search for SOMETHING\n",
    "    # so we'll start by just searching for recent articles.\n",
    "#     parser = QueryParser(\"published\", ix.schema)\n",
    "#     parser.add_plugin(DateParserPlugin())\n",
    "    # Let's limit the amount of sorting we have to do by only looking\n",
    "    # at pieces in recent history\n",
    "#     search_term = \"'-26 weeks to now'\"\n",
    "#     query = parser.parse(search_term)\n",
    "    results = searcher.search(query,\n",
    "        limit=None, sortedby=\"published\", reverse=True)\n",
    "\n",
    "    # Convert each Hit into a dict\n",
    "    def extract_hit_info(hit):\n",
    "        return {\n",
    "            'title': hit.get('title'),\n",
    "            'publication': hit.get('publication'),\n",
    "            'author': hit.get('author'),\n",
    "            'url': hit.get('url'),\n",
    "            'published': hit.get('published'),\n",
    "            'content_word_count': hit.get('content_word_count'),\n",
    "            'score': hit.score\n",
    "        }\n",
    "\n",
    "    hit_list = [extract_hit_info(h) for h in results]\n",
    "    print(hit_list)\n",
    "    print([h['content_word_count'] for h in hit_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
